{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allmore0/app_pprototipico/blob/main/PP_sesgo_min.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Importando los módulos de Python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "#import pymongo\n",
        "\n",
        "file_name = \"/content/Base Candidatos creada por IA (Gemini).csv\"\n",
        "df = pd.read_csv(file_name)"
      ],
      "metadata": {
        "id": "9KzbWxo5Qbso",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "0549031c-457d-469a-d0f4-f537b9bcbd4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Base Candidatos creada por IA (Gemini).csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1326571620.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/Base Candidatos creada por IA (Gemini).csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Base Candidatos creada por IA (Gemini).csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Expectativas salariales (usando el punto medio) - Usado solo por CNN\n",
        "\n",
        "df['Salario_Medio_MXN'] = pd.to_numeric(df['Sueldo_mensual'].astype(float))"
      ],
      "metadata": {
        "id": "8AWjaJsiQod2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODELO CNN PARA PREDICCIÓN DE NSE (AMAI)\n",
        "\n",
        "# Definición y mapeo de la Variable Objetivo (y): NSE (AMAI)\n",
        "# Mapeo de categorías a enteros (de menor a mayor)\n",
        "nse_mapping = {\n",
        "    'E': 0, 'D': 1, 'Dmas': 2,\n",
        "    'Cmenos': 3, 'C': 4, 'Cmas': 5,\n",
        "    'B': 6, 'A': 7\n",
        "}\n",
        "\n",
        "# Columna original para el mapeo y columna renombrada para el BIAS\n",
        "df['NSE_Mapped'] = df['Nivel_Socio_Económico(NSE_AMAI)'].map(nse_mapping).fillna(-1)\n",
        "df = df[df['NSE_Mapped'] != -1]\n",
        "unique_classes = sorted(df['NSE_Mapped'].unique())\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
        "df['y'] = df['NSE_Mapped'].map(class_to_idx)\n",
        "\n",
        "NUM_CLASSES = len(unique_classes)\n",
        "candidate_target_names = [name for name, val in nse_mapping.items() if val in unique_classes]\n",
        "print(f\"Clases únicas de NSE (AMAI) encontradas: {candidate_target_names}\")\n",
        "print(f\"Número de clases: {NUM_CLASSES}\")\n",
        "\n",
        "df.rename(columns={\n",
        "    'Años de experiencia': 'Anios_de_experiencia',\n",
        "    'Título_Principal': 'Titulo_Principal',\n",
        "    'Certificación_1': 'Certificacion_1',\n",
        "    'Certificación_2': 'Certificacion_2',\n",
        "    'Estadística_Avanzada_Porcentaje': 'Estadistica_Avanzada_Pct',\n",
        "    'Python_Porcentaje': 'Python_Pct',\n",
        "    'R_Porcentaje': 'R_Pct',\n",
        "    'SQL_Porcentaje': 'SQL_Pct',\n",
        "    'Nivel_Socio_Económico(NSE_AMAI)': 'NSE_AMAI_Bias', # para el resumen de sesgos\n",
        "    'Etnia_(Autodefinición)': 'Etnia_Autodefinicion'\n",
        "}, inplace=True)"
      ],
      "metadata": {
        "id": "eKbsDpUuVF3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = [\n",
        "    'Anios_de_experiencia', 'Edad', 'Python_Pct', 'R_Pct', 'SQL_Pct',\n",
        "    'Estadistica_Avanzada_Pct', 'Salario_Medio_MXN'\n",
        "]\n",
        "categorical_features = ['Género'] # Usamos la técnica One-Hot Encoding para que se decifren numéricamente las categorías\n",
        "df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
        "X_cols = numerical_features + [col for col in df_encoded.columns if any(cat_col in col for cat_col in categorical_features) and col not in categorical_features]\n",
        "# Valido columnas\n",
        "X = df_encoded[X_cols].values\n",
        "y = df_encoded['y'].values\n",
        "NUM_FEATURES = X.shape[1]\n",
        "print(f\"Número de características finales: {NUM_FEATURES}\")\n",
        "\n",
        "# Final del preproceso para CNN\n",
        "y_categorical = to_categorical(y, num_classes=NUM_CLASSES)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_reshaped = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_reshaped, y_categorical, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "dWFt74cJYDg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento del modelo CNN\n",
        "# Defino el modelo CNN (input_shape adaptado a NUM_FEATURES)\n",
        "modelo = Sequential([\n",
        "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(NUM_FEATURES, 1)),\n",
        "    Conv1D(64, kernel_size=3, activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(NUM_CLASSES, activation='softmax') # Ajustado a NUM_CLASSES\n",
        "])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Comeinza el entrenamiento\n",
        "print(\"\\nComenzando el entrenamiento del modelo...\")\n",
        "history = modelo.fit(X_train, y_train, epochs=80, batch_size=8,\n",
        "                     validation_data=(X_test, y_test), verbose=0)\n",
        "print(\"Entrenamiento finalizado.\")"
      ],
      "metadata": {
        "id": "5DMMf4TiZXDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación del modelo CNN\n",
        "plt.figure(figsize=(12, 4))\n",
        "loss, acc = modelo.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Evaluación del Modelo CNN\")\n",
        "print(f\"Loss en Test: {loss:.4f} - Accuracy en Test: {acc:.4f}\")\n",
        "\n",
        "# Matriz de confusión para el NSE (AMAI)\n",
        "pred = modelo.predict(X_test)\n",
        "y_pred = np.argmax(pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Sensitivity (Recall) por clase\n",
        "sensitivity = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
        "print(\"Sensibilidad (Recall) por Clase\")\n",
        "for i, s in enumerate(sensitivity):\n",
        "    print(f\"{candidate_target_names[i]}: {s:.2f}\")"
      ],
      "metadata": {
        "id": "Yeua98FfaLsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicción de un nuevo candidato\n",
        "# Candidato de ejemplo con valores altos para probar la predicción\n",
        "data_new = {col: 0 for col in X_cols}\n",
        "data_new.update({\n",
        "    'Anios_de_experiencia': 8.0,\n",
        "    'Edad': 38,\n",
        "    'Python_Pct': 0.98,  # Valores ya están en [0, 1]\n",
        "    'R_Pct': 0.95,\n",
        "    'SQL_Pct': 0.92,\n",
        "    'Estadistica_Avanzada_Pct': 0.96,\n",
        "    'Salario_Medio_MXN': 110000.0,\n",
        "    # Categorical features for 'Masculino' and 'Prioritario'\n",
        "    'Género_Masculino': 1,\n",
        "    'Estatus_Prioritario': 1,\n",
        "})\n",
        "nuevo_candidato_df = pd.DataFrame([data_new], columns=X_cols).fillna(0)\n",
        "\n",
        "# Procesamiento y Predicción\n",
        "nuevo_candidato_scaled = scaler.transform(nuevo_candidato_df.values)\n",
        "nuevo_candidato_reshaped = nuevo_candidato_scaled.reshape(1, NUM_FEATURES, 1)\n",
        "pred_nuevo = modelo.predict(nuevo_candidato_reshaped, verbose=0)\n",
        "clase_predicha_idx = np.argmax(pred_nuevo)\n",
        "\n",
        "print(\"<<<<< Evaluación de Nuevo Candidato CNN >>>>>>\")\n",
        "print(f\"Puntuación de la predicción (Softmax): {pred_nuevo[0]}\")\n",
        "print(f\"Resultado: {candidate_target_names[clase_predicha_idx]}\")"
      ],
      "metadata": {
        "id": "EOboJpB2akG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluaciones de los resultados de candidatos (SCORING) con mitigación de sesgo\n",
        "# Definiendo mappings y las funciones de puntuación\n",
        "# A) Para los títulos principales\n",
        "def score_titulo(titulo):\n",
        "    titulo = str(titulo).lower()\n",
        "    if 'ph.d.' in titulo or 'doctorado' in titulo or 'ia' in titulo:\n",
        "        return 1.0\n",
        "    elif 'maestría' in titulo or 'master' in titulo:\n",
        "        return 0.8\n",
        "    elif 'lic.' in titulo or 'ing.' in titulo or 'matemáticas' in titulo or 'computación' in titulo or 'ciencias de datos' in titulo:\n",
        "        return 0.6\n",
        "    else:\n",
        "        return 0.3\n",
        "\n",
        "# B) Certificaciones clave\n",
        "def score_certificaciones(cert1, cert2):\n",
        "    score = 0\n",
        "    keywords = ['ml', 'ai', 'data', 'cloud', 'aws', 'azure', 'gcp', 'cert', 'specialty', 'recomendación']\n",
        "    certs = [str(cert1).lower(), str(cert2).lower()]\n",
        "    for cert in certs:\n",
        "        if any(k in cert for k in keywords):\n",
        "            score += 0.5 # 0.5 por certificación relevante, max 1.0\n",
        "    return min(score, 1.0)\n",
        "\n",
        "# C) Idiomas\n",
        "level_map = {\n",
        "    'a1': 0.1, 'a2': 0.2, 'b1': 0.4, 'b2': 0.6, 'c1': 0.8, 'c2': 1.0\n",
        "}\n",
        "\n",
        "def score_idiomas(nivel1, nivel2):\n",
        "    score1 = level_map.get(str(nivel1).lower(), 0)\n",
        "    score2 = level_map.get(str(nivel2).lower(), 0)\n",
        "    # Sumar y normalizar a un resultado (score) máximo de 1.0\n",
        "    return (score1 + score2) / 2.0\n",
        "\n",
        "# Aplicamos funciones de puntuación al DataFrame\n",
        "df['Score_Titulo'] = df['Titulo_Principal'].apply(score_titulo)\n",
        "df['Score_Certificaciones'] = df.apply(lambda row: score_certificaciones(row['Certificacion_1'], row['Certificacion_2']), axis=1)\n",
        "df['Score_Idiomas'] = df.apply(lambda row: score_idiomas(row['Nivel_idioma_1'], row['Nivel_idioma_2']), axis=1)\n",
        "\n",
        "# 2. Cálculo del resultado (score) final ponderado\n",
        "weights = {\n",
        "    'Python_Pct': 0.25,\n",
        "    'SQL_Pct': 0.20,\n",
        "    'Estadistica_Avanzada_Pct': 0.15,\n",
        "    'R_Pct': 0.05,\n",
        "    'Score_Titulo': 0.15,\n",
        "    'Score_Certificaciones': 0.10,\n",
        "    'Score_Idiomas': 0.10\n",
        "}\n",
        "\n",
        "df['Score_Base'] = (\n",
        "    df['Python_Pct'] * weights['Python_Pct'] +\n",
        "    df['SQL_Pct'] * weights['SQL_Pct'] +\n",
        "    df['Estadistica_Avanzada_Pct'] * weights['Estadistica_Avanzada_Pct'] +\n",
        "    df['R_Pct'] * weights['R_Pct'] +\n",
        "    df['Score_Titulo'] * weights['Score_Titulo'] +\n",
        "    df['Score_Certificaciones'] * weights['Score_Certificaciones'] +\n",
        "    df['Score_Idiomas'] * weights['Score_Idiomas']\n",
        ")\n",
        "\n",
        "# Ajuste del resultado (score) por los años de experiencia (mitigación de sesgo a la edad/años)\n",
        "df['Experiencia_Multiplier'] = 1 + np.log1p(df['Anios_de_experiencia']) * 0.05\n",
        "df['Score_Final'] = df['Score_Base'] * df['Experiencia_Multiplier']\n",
        "\n",
        "# 3. Identificación del mejor candidato y Resumen de sesgos\n",
        "best_candidate = df.loc[df['Score_Final'].idxmax()]\n",
        "\n",
        "# Resumen de eliminación de sesgos\n",
        "bias_cols = ['Edad', 'Género', 'Religión_ficticia', 'Afiliación_política_ficticia', 'NSE_AMAI_Bias', 'Etnia_Autodefinicion']\n",
        "top_n = 10\n",
        "top_candidates = df.sort_values(by='Score_Final', ascending=False).head(top_n)\n",
        "\n",
        "print(\"\\n\\n*******************************************************\")\n",
        "print(\"\\U0001F9D1\\U0000200D\\U0001F4BB\\U0001F947 Candidato con Mejor Puntuación Final (Data Scientist Score):\")\n",
        "print(\"*******************************************************\")\n",
        "print(f\"ID: {best_candidate['ID']}\")\n",
        "print(f\"Nombre: {best_candidate['Nombre(s)']} {best_candidate['Apellido_Paterno']} {best_candidate['Apellido_Materno']}\")\n",
        "print(f\"Años de Experiencia: {best_candidate['Anios_de_experiencia']}\")\n",
        "print(f\"Score Final: {best_candidate['Score_Final']:.4f}\")\n",
        "\n",
        "print(\"\\n\\n*******************************************************\")\n",
        "print(\"\\u2696\\ufe0f Resumen de Eliminación de Sesgos (Comparación Top 10 vs. Población Total):\")\n",
        "print(\"*******************************************************\")\n",
        "for col in bias_cols:\n",
        "    # Distribución en la población total\n",
        "    total_dist = df[col].value_counts(normalize=True).mul(100).round(2)\n",
        "    total_dist.name = 'Poblacion Total (%)'\n",
        "\n",
        "    # Distribución en el top N\n",
        "    top_dist = top_candidates[col].value_counts(normalize=True).mul(100).round(2)\n",
        "    top_dist.name = f'Top {top_n} Seleccionados (%)'\n",
        "\n",
        "    # Unión de las distribuciones y cálculo la 'Diferencia'\n",
        "    summary = pd.concat([total_dist, top_dist], axis=1)\n",
        "    summary.fillna(0, inplace=True)\n",
        "    summary['Diferencia Absoluta (%)'] = (summary[f'Top {top_n} Seleccionados (%)'] - summary['Poblacion Total (%)']).round(2)\n",
        "\n",
        "    print(f\"\\nColumna: {col}\")\n",
        "    print(summary.to_string())\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Guardar el DataFrame con los scores finales\n",
        "output_df = df[['ID', 'Nombre(s)', 'Apellido_Paterno','Apellido_Materno', 'Anios_de_experiencia', 'Score_Final']].sort_values(by='Score_Final', ascending=False)\n",
        "output_df.to_csv('candidatos_evaluados_score_final.csv', index=False)\n",
        "print(\"\\nEl archivo 'candidatos_evaluados_score_final.csv' ha sido generado con los resultados finales.\")"
      ],
      "metadata": {
        "id": "o4Q3x8rYa55Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1133a0be"
      },
      "source": [
        "# Task\n",
        "- **Add MongoDB Import**: Add the `import pymongo` statement to the initial imports cell.\n",
        "- **Load Web Form Data**: Load the data from the 'web_form_data.json' file into a temporary pandas DataFrame.\n",
        "- **Append New Data**: Append the loaded web form data to the existing `df` DataFrame.\n",
        "- **Final Task**: Confirm the MongoDB module import and the successful appending of web form data to the DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "badcb5fe"
      },
      "source": [
        "## Add MongoDB Import\n",
        "\n",
        "### Subtask:\n",
        "Add the `import pymongo` statement to the initial imports cell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73a10add"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires adding `import pymongo` to the initial imports cell. I will update the content of the first code cell to include this new import statement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22745088"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to import `pymongo` failed because the module was not found. This indicates that the `pymongo` library needs to be installed in the environment. I will add a `!pip install pymongo` command in a new cell before the import statement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "266c6bdd"
      },
      "source": [
        "!pip install pymongo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b92ea64e"
      },
      "source": [
        "**Reasoning**:\n",
        "The `pymongo` library has been successfully installed in the previous step. Now, I will re-execute the initial imports cell, which now includes the `import pymongo` statement, to ensure all necessary modules are loaded without error.\n",
        "\n"
      ]
    }
  ]
}